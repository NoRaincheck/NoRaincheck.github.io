## A quick look at `onnxscript`

_August 2025_

[`onnxscript`](https://github.com/microsoft/onnxscript) is a weird project that I've been keeping tabs on. It's weird because its like an ORM. It's functionally useless if you aren't familiar with lower level `onnx` concepts or don't know how to construct an `onnx` graph from low level primitives, in that debugging will be a nightmare, and yet there are a lot of abstractions that safe you a lot of time and effort. 

For example, imagine you want to compute cosine similarity:

```py
import numpy as np
import onnx
import onnx.helper as oh
import onnxruntime

init = [
    oh.make_tensor("depth", onnx.TensorProto.INT64, [], [max_vocab_size]),
    oh.make_tensor("values", onnx.TensorProto.FLOAT, [2], [0.0, 1.0]),
    oh.make_tensor("axes_1", onnx.TensorProto.INT64, [1], [1]),
    oh.make_tensor("two", onnx.TensorProto.FLOAT, [1], [2.0]),
]


inputs = [
    oh.make_tensor_value_info("input_ids", onnx.TensorProto.INT64, [None, None]),
]

nodes = []

nodes.append(oh.make_node("OneHot", ["input_ids", "depth", "values"], ["one_hot_ids"]))
nodes.append(oh.make_node("ReduceSum", ["one_hot_ids", "axes_1"], ["reduce_sum_ids"], keepdims=0))

# do cosine similarity of the self matrix
# Compute cosine similarity using lower-level ONNX ops

# 1. Compute dot product: sum(reduce_sum_ids * reduce_sum_ids, axis=1, keepdims=True)
nodes.append(oh.make_node("Transpose", ["reduce_sum_ids"], ["transpose_ids"]))
nodes.append(oh.make_node("MatMul", ["reduce_sum_ids", "transpose_ids"], ["dot_product"]))

# 2. Compute L2 norm: sqrt(sum(reduce_sum_ids ** 2, axis=1, keepdims=True))
# Add a constant for 2.0
nodes.append(oh.make_node("Pow", ["reduce_sum_ids", "two"], ["squared_ids"]))
nodes.append(oh.make_node("ReduceSum", ["squared_ids", "axes_1"], ["sum_squared"], keepdims=1))
nodes.append(oh.make_node("Sqrt", ["sum_squared"], ["norm"]))

# 3. Compute cosine similarity: dot_product / (norm * norm)
nodes.append(oh.make_node("Transpose", ["norm"], ["transpose_norm"]))
nodes.append(oh.make_node("Mul", ["norm", "transpose_norm"], ["norm_product"]))
nodes.append(oh.make_node("Div", ["dot_product", "norm_product"], ["cosine_similarity_ids"]))

outputs = [oh.make_tensor_value_info("cosine_similarity_ids", onnx.TensorProto.FLOAT, [None, None])]
graph = oh.make_graph(nodes, "cosine_similarity_graph", inputs, outputs, initializer=init)

model = oh.make_model(
    graph,
    producer_name="cosine_similarity_model",
    ir_version=10,
    opset_imports=[oh.make_opsetid("", 18)],
)

session = onnxruntime.InferenceSession(model.SerializeToString())

input_ids = np.array([[1, 2, 3], [1, 1, 2]]).astype(np.int64)

output = session.run(
    None,
    {
        "input_ids": input_ids,
    },
)
print(output)
```

Theres a lot going on there, and its not particularly re-useable, testable or sane to parse though the graph construction. In `onnxscript` you can compose things in a more sane manner:

```py
from onnxscript import FLOAT, script

from onnxscript_mods.config import op, op_onnxscript_mods


@script(op_onnxscript_mods, default_opset=op)
def linalg_norm(x: FLOAT):
    norm = op.Pow(x, 2.0)
    norm = op.ReduceSum(norm, axes=[1], keepdims=1)
    norm = op.Sqrt(norm)
    return norm


@script(op_onnxscript_mods, default_opset=op)
def cosine_similarity(x: FLOAT):
    xt = op.Transpose(x)
    dot = op.MatMul(x, xt)
    norm_value = linalg_norm(x)
    norm_t = op.Transpose(norm_value)
    norm_product = op.Mul(norm_value, norm_t)
    return op.Div(dot, norm_product)

# Usage
cosine_similarity(input_ids)
```

This is much more readable (and if you compose your `onnx` file using `.to_model_proto(functions=[linalg_norm])`, the `onnx` graph would be cleaner as well). 

However the ergonomics of some aspects are still a bit weird. For example, if you use the community extensions from `com.microsoft` domain, it can infer the `model.proto`, but since the `onnxscript` library doens't _know_ about the extensions, you need to rely on the `onnxruntime` to execute things. (Also debugging is extremely painful). For example, here I'm building trigrams, making use of the `Tokenizer` custom operator:

```py
com_microsoft = Opset("com.microsoft", 1)

@script(com_microsoft, default_opset=op, ir_version=7)
def trigrams(s: STRING[None]):  # pyrefly: ignore
    """
    Generate a trigram from a string.
    """
    characters = op.Squeeze(
        com_microsoft.Tokenizer(s, tokenexp=".", mincharnum=1, mark=0, pad_value=""), op.Constant(value_ints=[0])
    )
    # get the shape
    shape = op.Shape(characters)
    # get the last dimension
    last_dim = op.Gather(shape, indices=[-1], axis=0)
    last_dim_minus_1 = op.Sub(last_dim, op.Constant(value_int=1))
    last_dim_minus_2 = op.Sub(last_dim_minus_1, op.Constant(value_int=1))

    return op.StringConcat(
        op.StringConcat(
            op.GatherElements(
                characters, op.Range(op.Constant(value_int=0), last_dim_minus_2, op.Constant(value_int=1)), axis=0
            ),
            op.GatherElements(
                characters, op.Range(op.Constant(value_int=1), last_dim_minus_1, op.Constant(value_int=1)), axis=0
            ),
        ),
        op.GatherElements(characters, op.Range(op.Constant(value_int=2), last_dim, op.Constant(value_int=1)), axis=0),
    )
```

This works well enough, but since `com_microsoft = Opset("com.microsoft", 1)` is a custom `Opset`, then `trigram(np.array(["Hello World"]))` would not work. There are definitely some meta-programming ideas that can be used effectively in `onnxscript` however its still pretty rough on the edges. It's definitely something I'll keep eyes on in the future especially since `torch`'s dynamo `onnx` exporter is built ontop of `onnxscript`.

**Extension**

If you're okay with fragile-ness in the `ModelProto` to `FunctionProto` conversion, one could incorporate arbitrary `onnx` files with `onnxscript`. It would look something like this:

```py
import numpy as np
import onnx
import onnx_ir
import onnxruntime
from onnxscript import FLOAT, script
from onnxscript import opset15 as op
from onnxscript.values import Opset

# A dummy opset used for model-local functions
local = Opset("local", 1)


@script(local, default_opset=op)
def diff_square(x, y):
    diff = x - y
    return diff * diff


@script(local)
def sum(z):
    return op.ReduceSum(z, keepdims=1)


@script()
def l2norm(x: FLOAT["N"], y: FLOAT["N"]) -> FLOAT[1]:  # noqa: F821
    return op.Sqrt(sum(diff_square(x, y)))


@script()
def l2norm2(x: FLOAT["N"], y: FLOAT["N"]) -> FLOAT[1]:  # noqa: F821
    return op.Sqrt(local.sum(local.diff_square(x, y)))


def convert_model_to_function(model: onnx.ModelProto, domain, name) -> onnx.FunctionProto:
    model_ir = onnx_ir.serde.deserialize_model(model)
    function_ir = onnx_ir.Function(domain=domain, name=name, graph=model_ir.graph, attributes={})
    return onnx_ir.to_proto(function_ir)


a_non_onnxscript_function = convert_model_to_function(sum.to_model_proto(), "local", "sum")


model = l2norm2.to_model_proto(functions=[a_non_onnxscript_function, diff_square])
print(onnx.printer.to_text(model))


session = onnxruntime.InferenceSession(model.SerializeToString())
print(
    session.run(
        None, {"x": np.array([1.0, 2.0, 3.0]).astype(np.float32), "y": np.array([4.0, 5.0, 6.0]).astype(np.float32)}
    )
)
```

